### LXD Lab Ortamı Kurulumu

LXD'nin kurulumu ve ilk konfigürasyonunu yaptıktan sonra Kubernetes için lab ortamı kurulumu ile devam edeceğiz.

**Ubuntu için kurulum**
```shell
sudo snap install lxd
```

**Centos için kurulum**
```shell
sudo yum install epel-release
sudo yum install snapd
sudo systemctl enable --now snapd.socket
sudo ln -s /var/lib/snapd/snap /snap
```

Aktif kullanıcımızı LXD grubuna dahil ederek root ile çalışmayı bırakıyoruz.

```shell
sudo usermod -a -G lxd $(whoami)
newgrp lxd
```

LXD'yi konfigüre ediyoruz. Alltaki komutu çalıştırdığınızda cli sırasıyla size bzı dorular soracak. Aşağıdaki taploda açıklamaları vermeinz gereken cevaplar yer alıyor.

```shell
lxd init
```

|Özellik|Açıklama|Opsiyonlar|Detaylar|
|-------|--------|----------|--------|
|Clustering|LXC cluster aktivasyonu|varsayılan değeri no'dur. Eğer yes seçilirse ya hazır bir cluster'a katılmak yada yeni bir cluster oluşturmak geekecektir. Biz no olarak varsayılan değerle devam ediyoruz.| Daha fazla detay için [şu sayfayı](https://linuxcontainers.org/lxd/docs/master/clustering.html) ziyaret ediniz.|
|MAAS server|Fiziksel hardware üzerine bulut sistem kurmaya yarayan ve  Canonical tarafından geliştirilen bir araç| varsayılan olarak no seçilidir. Yani bir MAAS servise bağlanmak itemiyoruz| Daha fazla detay için [şu sayfayı](https://maas.io/) ve (şu sayfayı)[https://linuxcontainers.org/lxd/docs/master/instances#type-nic] ziyaret ediniz.|
|Network bridge|konteynerlar için network sağlar.|Yeni bridge network oluşturmak tavsiye edilen yöntem.| Daha fazla detay için [şu sayfayı](https://linuxcontainers.org/lxd/docs/master/networks.html) ve [şu sayfayı](https://linuxcontainers.org/lxd/docs/master/instances#type-nic) ziyaret ediniz.|
|Storage pools|Depolama havuzunun belirlenmesi|Production ortamı için loop-backed storage yerine gerçek disk veya zfs, btrfs yada dağınık storage sistemlerinden Ceph, GlusterFs gibi sistemler kullanak tavsiye edi,len çözümler. Ancak biz burada lokal makinamızda çalıştığımız için "dir" seçeneğini kullanıyoruz.|Daha fazla detay için  şu kaynaklara bakınız. [Link1](https://linuxcontainers.org/lxd/docs/master/storage.html),[Link2](https://linuxcontainers.org/lxd/docs/master/storage.html#where-to-store-lxd-data),[Link3](https://linuxcontainers.org/lxd/docs/master/storage#feature-comparison)|
|Network Access|LXC'nin network üzerinden erişime açılıp açılmamasına izin verir.| default değer no ve bizde böyle seçiyoruz.|-| 
|Automatic Image Update|Image otomatik update olsun mu?|Varsayılan değer yes ve bizde öyle devam ediyoruz.|-|
|"YAML lxd init preseed"|yapılan ayalar yaml olarak kaydedilsin mi?|default değer yes ancak siz istediğiniz gibi seçebilirsiniz.|-|

Kurulumu kontrol için alttaki komutu kullanıyoruz.
```shell
lxc version
```

Birde Ubuntu image'ı çalıştırarak test edelim.

```shell
lxc launch ubuntu:21.04 myubuntu
```
Konteyner'a giriş yapıyoruz. 

```
lxc exec myubuntu bash
```
#### Kuernetes için LXC konteynerların Oluşturulması


Başlamadan önce topolojiyi hatırlayalım. Amacımız alttaki 8 makinayı ve bunların network bağlantılarını oluşturmak.

![topoloji.jpg](files/topoloji.jpg)



**UYARI:** LXD içinde Kubernetes çalıştıracağımız için LXC instance'larının nested container çalıştırmaya hazır hale getirilmesi gerekiyor. Aşağıda yapacağımız konfigürasyonlarda bu konu dikkate alındı. Ancak yine de kurulum esnasında sorunlarla karşılaşabilirsiniz. Bu nedenle kuruluma başlamadan önce [şu sayfayı](https://github.com/corneliusweig/kubernetes-lxd) kesinlikle ziyaret edin.

Öncelikle cluster için gerekli olan network'ü oluşturuyoruz.

```shell
lxc network create kubebridge --type=bridge \
        ipv4.address="10.240.10.1/24" \
        bridge.driver="native" \
        ipv6.address=none ipv4.nat=true \
        ipv4.dhcp=true
```
Daha sonra host makinamızda bütün makinalarımızda kullanacağımız profili oluşturuyoruz. Üstte bahsettiğim sayfaya göz gezdirdiyseniz profil dosyasındaki konfigürasyonları da görebilirsiniz.

_limits.memory.swap: "false"_ LXC içinde swap'ı disabled yapıyor. Kubelet swap enabled iken çalışmıyor. 


```
lxc profile create kubernetes

cat > kubernetes.profile <<EOF
config:
  limits.cpu: "1"
  limits.memory: 2GB
  limits.memory.swap: "false"
  linux.kernel_modules: br_netfilter,ip_tables,ip6_tables,netlink_diag,nf_nat,overlay
  raw.lxc: "lxc.apparmor.profile=unconfined\nlxc.cap.drop= \nlxc.cgroup.devices.allow=a\nlxc.mount.auto=proc:rw sys:rw"
  security.nesting: "true"
  security.privileged: "true"
description: Kubernetes profile
devices:
  root:
    path: /
    pool: default
    type: disk
name: kubernetes
EOF

lxc profile edit kubernetes < kubernetes.profile
```

Daha sonra makinalarımızı oluşturuyoruz.


```
lxc init ubuntu:21.04 lb-01 -p kubernetes
lxc init ubuntu:21.04 lb-02 -p kubernetes

lxc init ubuntu:21.04 controller-01 -p kubernetes
lxc init ubuntu:21.04 controller-02 -p kubernetes
lxc init ubuntu:21.04 controller-03 -p kubernetes


lxc init ubuntu:21.04 worker-01 -p kubernetes
lxc init ubuntu:21.04 worker-02 -p kubernetes
lxc init ubuntu:21.04 worker-03 -p kubernetes



lxc network attach kubebridge lb-01 eth0                                                       
lxc network attach kubebridge lb-02 eth0

lxc network attach kubebridge controller-01 eth0
lxc network attach kubebridge controller-02 eth0
lxc network attach kubebridge controller-03 eth0

lxc network attach kubebridge worker-01 eth0
lxc network attach kubebridge worker-02 eth0
lxc network attach kubebridge worker-03 eth0


lxc config device set lb-01 eth0 ipv4.address 10.240.10.2
lxc config device set lb-02 eth0 ipv4.address 10.240.10.3

lxc config device set controller-01 eth0 ipv4.address 10.240.10.4
lxc config device set controller-02 eth0 ipv4.address 10.240.10.5
lxc config device set controller-03 eth0 ipv4.address 10.240.10.6

lxc config device set worker-01 eth0 ipv4.address 10.240.10.7
lxc config device set worker-02 eth0 ipv4.address 10.240.10.8
lxc config device set worker-03 eth0 ipv4.address 10.240.10.9


#lxc config set worker-01  linux.kernel_modules ip_tables,ip6_tables,netlink_diag,nf_nat,overlay,br_netfilter security.nesting "true"  security.privileged "true" 

#printf 'lxc.net.0.ipv4.address = 10.240.10.2/24\nlxc.apparmor.profile=unconfined\nlxc.cap.drop= \nlxc.cgroup.devices.allow=a\nlxc.mount.auto=proc:rw sys:rw' | lxc config set lb-01 raw.lxc -

printf 'lxc.net.0.ipv4.address = 10.240.10.2/24' | lxc config set lb-01 raw.lxc -
printf 'lxc.net.0.ipv4.address = 10.240.10.3/24' | lxc config set lb-02 raw.lxc -
başlamadan önce 
printf 'lxc.net.0.ipv4.address = 10.240.10.4/24' | lxc config set controller-01 raw.lxc -
printf 'lxc.net.0.ipv4.address = 10.240.10.5/24' | lxc config set controller-02 raw.lxc -
printf 'lxc.net.0.ipv4.address = 10.240.10.6/24' | lxc config set controller-03 raw.lxc -

printf 'lxc.net.0.ipv4.address = 10.240.10.7/24' | lxc config set worker-01 raw.lxc -
printf 'lxc.net.0.ipv4.address = 10.240.10.8/24' | lxc config set worker-02 raw.lxc -
printf 'lxc.net.0.ipv4.address = 10.240.10.9/24' | lxc config set worker-03 raw.lxc -
```

Örnek olarak worker-03 makinamızın config ayarları şu şekilde olmalıdır.

```
lxc config show  worker-03

# sonuç

architecture: x86_64
config:
  image.architecture: amd64
  image.description: ubuntu 21.04 amd64 (release) (20210603)
  image.label: release
  image.os: ubuntu
  image.release: hirsute
  image.serial: "20210603"
  image.type: squashfs
  image.version: "21.04"
  limits.cpu: "2"
  linux.kernel_modules: br_netfilter,ip_tables,ip6_tables,netlink_diag,nf_nat,overlay
  raw.lxc: |
    lxc.net.0.ipv4.address = 10.240.10.9/24
    lxc.apparmor.profile=unconfined
    lxc.mount.auto=proc:rw sys:rw cgroup:rw
    lxc.cgroup.devices.allow=a
    lxc.cap.drop=
  security.nesting: "true"
  security.privileged: "true"
  volatile.base_image: 175031a4bcde6881fe6580b029bd481b1ea2d3c2ab5ff67bc4df1c4f2f3c9c41
  volatile.eth0.host_name: vethea1f7d4c
  volatile.eth0.hwaddr: 00:16:3e:56:4f:c1
  volatile.eth0.name: eth0
  volatile.idmap.base: "0"
  volatile.idmap.current: '[]'
  volatile.idmap.next: '[]'
  volatile.last_state.idmap: '[]'
  volatile.last_state.power: RUNNING
  volatile.uuid: 208225e0-7b90-4772-bec8-023d736b3101
devices:
  eth0:
    ipv4.address: 10.240.10.9
    network: kubebridge
    type: nic
ephemeral: false
profiles:
- kubernetes
stateful: false
description: ""
```

Son olarak LXC konteynerlarımızı çalıştırıyoruz.

```shell
lxc start lb-01 lb-02 controller-01 controller-02 controller-03  worker-01 worker-02  worker-03
```
Kurulum tamamlandığında makinalarımız aşağıdaki gibi görünüyor olacak.

```
lxc ls
+---------------+---------+--------------------+------+-----------+-----------+
|     NAME      |  STATE  |        IPV4        | IPV6 |   TYPE    | SNAPSHOTS |
+---------------+---------+--------------------+------+-----------+-----------+
| controller-01 | RUNNING | 10.240.10.4 (eth0) |      | CONTAINER | 0         |
+---------------+---------+--------------------+------+-----------+-----------+
| controller-02 | RUNNING | 10.240.10.5 (eth0) |      | CONTAINER | 0         |
+---------------+---------+--------------------+------+-----------+-----------+
| controller-03 | RUNNING | 10.240.10.6 (eth0) |      | CONTAINER | 0         |
+---------------+---------+--------------------+------+-----------+-----------+
| lb-01         | RUNNING | 10.240.10.2 (eth0) |      | CONTAINER | 0         |
+---------------+---------+--------------------+------+-----------+-----------+
| lb-02         | RUNNING | 10.240.10.3 (eth0) |      | CONTAINER | 0         |
+---------------+---------+--------------------+------+-----------+-----------+
| worker-01     | RUNNING | 10.240.10.7 (eth0) |      | CONTAINER | 0         |
+---------------+---------+--------------------+------+-----------+-----------+
| worker-02     | RUNNING | 10.240.10.8 (eth0) |      | CONTAINER | 0         |
+---------------+---------+--------------------+------+-----------+-----------+
| worker-03     | RUNNING | 10.240.10.9 (eth0) |      | CONTAINER | 0         |
+---------------+---------+--------------------+------+-----------+-----------+

```

Bütün makinalarda update ve upgrade işlemlerini yapıyoruz. Örnek olarak lb-01 makinasına giriş yapmak için alttak komutu kullanabilirsiniz.
```
lxc exec lb-01 bash
```

**Ubuntu için:**

Update'i çalıştırıyoruz
```
apt update && apt upgrade
```
**Centos için**
```
yum upgrade
```
Makinaları birbirine ping atabildiğinden emin olmak için aşağıdaki örnek komutla bütün makinaları test edebilirsiniz.

```
lxc exec lb-01 -- ping 10.240.10.6 -c 2
```

- [Giriş ve Lab Ortamının Kurulması](1.Giris.md)
  - [LXD ile Lab Ortamı Kurulumu](2.LXD-Lab.md)
  - [KVM-Qemu İle Lab Ortamı Kurulumu](3.KVM-Qemu-Lab.md)
  - [Vagrant-Libvirt ile Lab Ortamı Kurulumu](4.Vagrant-Libvirt-Lab.md)
  - [Vagrant-Virtualbox ile Lab Ortamı Kurulumu](5.Vagrant-Virtualbox-Lab.md)
- [İstemci Araçlarının Kurulumu](6.Host-Client-Tools.md)
- [Harici Yük Dengeleyici Kurulumu](7.External-LB.md)
- [Certificate Authority ve TLS Sertifikalarının Oluşturulması](8.CA-TLS.md)
- [Kubeconfig Dosyalarının Oluşturulması](9.Kubeconfig.md)
- [Data Encryption Config ve Key Oluşturulması](10.Data-Encryption.md)
- [Etcd Cluster Kurulumu](11.ETCD.md)
- [Control Plane'nin Kurulumu ve Konfigürasyonu](12.Control-Plane.md)
- [Worker Node'ların Kurulumu ve Konfigürasyonu](13.Worker-Node.md)
- [Uzak Bağlantı için Kubectl Konfigürasyonu](14.Kubectl.md)
- [Pod Network Route'larının Ayarlanması](14.Pod-Network.md)
- [DNS'in Kurulumu ve Konfigürasyonu](15.DNS-Addon.md)
- [Smoke Test](16.Smoke-Test.md) 

## Kaynaklar
- https://github.com/cloud-helpers/kubernetes-hard-way-bare-metal/blob/master/lxc/README.md
- https://www.nicktriller.com/blog/notes-on-kubernetes-the-hard-way/
- https://github.com/rgmorales/kubernetes-the-hard-way-on-lxd
- https://github.com/kelseyhightower/kubernetes-the-hard-way